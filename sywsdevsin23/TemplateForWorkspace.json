{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "sywsdevsin23"
		},
		"sywsdevsin23-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sywsdevsin23-WorkspaceDefaultSqlServer'"
		},
		"Ls_AdlsGen2_01_properties_typeProperties_url": {
			"type": "object",
			"defaultValue": {
				"type": "AzureKeyVaultSecret",
				"store": {
					"referenceName": "Ls_KeyVault_01",
					"type": "LinkedServiceReference"
				},
				"secretName": "datalakeurl"
			}
		},
		"Ls_KeyVault_01_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://mdwdops-kv-dev-sin23.vault.azure.net/"
		},
		"Ls_Rest_MelParkSensors_01_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://data.melbourne.vic.gov.au/resource/"
		},
		"sywsdevsin23-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://mdwdopsst2devsin23.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/P_Ingest_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Set infilefolder",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "infilefolder",
							"value": {
								"value": "@utcnow('yyyy_MM_dd_hh_mm_ss')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "DownloadSensorData",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set infilefolder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Ds_REST_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"relativeurl": "dtpv-d4pf.json"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Ds_AdlsGen2_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"infilefolder": "@variables('infilefolder')",
									"infilename": "MelbParkingSensorData.json",
									"container": "datalake/data/lnd"
								}
							}
						]
					},
					{
						"name": "DownloadBayData",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set infilefolder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Ds_REST_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"relativeurl": "wuf8-susg.json"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Ds_AdlsGen2_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"infilefolder": "@variables('infilefolder')",
									"infilename": "MelbParkingBayData.json",
									"container": "datalake/data/lnd"
								}
							}
						]
					},
					{
						"name": "StandardizeData",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "DownloadSensorData",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "DownloadBayData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "02_standardize",
								"type": "NotebookReference"
							},
							"parameters": {
								"infilefolder": {
									"value": {
										"value": "@variables('infilefolder')",
										"type": "Expression"
									},
									"type": "string"
								},
								"loadid": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								},
								"pipelinename": {
									"value": {
										"value": "@pipeline().Pipeline",
										"type": "Expression"
									},
									"type": "string"
								},
								"keyvaultlsname": {
									"value": "Ls_KeyVault_01",
									"type": "string"
								},
								"adls2lsname": {
									"value": "Ls_AdlsGen2_01",
									"type": "string"
								}
							}
						}
					},
					{
						"name": "TransformData",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "StandardizeData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "03_transform",
								"type": "NotebookReference"
							},
							"parameters": {
								"loadid": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								},
								"pipelinename": {
									"value": {
										"value": "@pipeline().Pipeline",
										"type": "Expression"
									},
									"type": "string"
								},
								"keyvaultlsname": {
									"value": "Ls_KeyVault_01",
									"type": "string"
								}
							}
						}
					},
					{
						"name": "Load SQL dedicated pool",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "TransformData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "syndpdevsin23",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[load_dw]",
							"storedProcedureParameters": {
								"load_id": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"infilefolder": {
						"type": "String",
						"defaultValue": "Ind"
					}
				},
				"annotations": [],
				"lastPublishTime": "2021-10-24T13:56:56Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Ds_REST_MelbParkingData')]",
				"[concat(variables('workspaceId'), '/datasets/Ds_AdlsGen2_MelbParkingData')]",
				"[concat(variables('workspaceId'), '/notebooks/02_standardize')]",
				"[concat(variables('workspaceId'), '/notebooks/03_transform')]",
				"[concat(variables('workspaceId'), '/sqlPools/syndpdevsin23')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ds_AdlsGen2_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Ls_AdlsGen2_01",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"infilefolder": {
						"type": "string"
					},
					"infilename": {
						"type": "string"
					},
					"container": {
						"type": "string",
						"defaultValue": "datalake/data/lnd"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().infilename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@concat(dataset().container, '/', dataset().infilefolder)",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Ls_AdlsGen2_01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ds_REST_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Ls_Rest_MelParkSensors_01",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativeurl": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@dataset().relativeurl",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Ls_Rest_MelParkSensors_01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ls_AdlsGen2_01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "AzureBlobFS",
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				},
				"annotations": [],
				"typeProperties": {
					"url": "[parameters('Ls_AdlsGen2_01_properties_typeProperties_url')]"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Ls_KeyVault_01')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ls_KeyVault_01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('Ls_KeyVault_01_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ls_Rest_MelParkSensors_01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Ls_Rest_MelParkSensors_01_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sywsdevsin23-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sywsdevsin23-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sywsdevsin23-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('sywsdevsin23-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/T_Sched')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "P_Ingest_MelbParkingData",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 24,
						"startTime": "2021-10-01T07:00:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/P_Ingest_MelbParkingData')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "synspdevsin23",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get variables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"keyvaultlsname = 'Ls_KeyVault_01'\r\n",
							"adls2lsname = 'Ls_AdlsGen2_01'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Linked Services Setup: KV and ADLS Gen2"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"storage_account = token_library.getSecretWithLS(keyvaultlsname, \"datalakeaccountname\")\r\n",
							"\r\n",
							"spark.conf.set(\"spark.storage.synapse.linkedServiceName\", adls2lsname)\r\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Create Schemas"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS dw LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\r\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS lnd LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\r\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS interim LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\r\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS malformed LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Create Fact Tables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS dw.fact_parking\")\r\n",
							"\r\n",
							"spark.sql(f\"CREATE TABLE dw.fact_parking(dim_date_id STRING,dim_time_id STRING, dim_parking_bay_id STRING, dim_location_id STRING, dim_st_marker_id STRING, status STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/fact_parking/'\")\r\n",
							" \r\n",
							"spark.sql(f\"REFRESH TABLE dw.fact_parking\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5. Create Dimension Tables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_st_marker\")\r\n",
							"spark.sql(f\"CREATE TABLE dw.dim_st_marker(dim_st_marker_id STRING, st_marker_id STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_st_marker/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_st_marker\")\r\n",
							" \r\n",
							"\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_location\")\r\n",
							"spark.sql(f\"CREATE TABLE dw.dim_location(dim_location_id STRING,lat FLOAT, lon FLOAT, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_location/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_location\")\r\n",
							" \r\n",
							"\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_parking_bay\")\r\n",
							"spark.sql(f\"CREATE TABLE dw.dim_parking_bay(dim_parking_bay_id STRING, bay_id INT,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_parking_bay/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_parking_bay\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 6. Create dim date and time"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_date\")\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_time\")\r\n",
							"\r\n",
							"# DimDate\r\n",
							"dimdate = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_date/dim_date.csv\", header=True)\r\n",
							"dimdate.write.saveAsTable(\"dw.dim_date\")\r\n",
							"\r\n",
							"# DimTime\r\n",
							"dimtime = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_time/dim_time.csv\", header=True)\r\n",
							"dimtime.write.saveAsTable(\"dw.dim_time\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 7. Create interim and error tables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS interim.parking_bay\")\r\n",
							"spark.sql(f\"CREATE TABLE interim.parking_bay(bay_id INT, `last_edit` TIMESTAMP, `marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.parking_bay/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE interim.parking_bay\")\r\n",
							"\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS interim.sensor\")\r\n",
							"spark.sql(f\"CREATE TABLE  interim.sensor(bay_id INT, `st_marker_id` STRING, `lat` FLOAT, `lon` FLOAT, `location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>, `status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.sensor/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE  interim.sensor\")\r\n",
							"   \r\n",
							"\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS malformed.parking_bay\")\r\n",
							"spark.sql(f\"CREATE TABLE malformed.parking_bay(bay_id INT, `last_edit` TIMESTAMP,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE malformed.parking_bay\")\r\n",
							"\r\n",
							"spark.sql(f\"DROP TABLE IF EXISTS malformed.sensor\")\r\n",
							"spark.sql(f\"CREATE TABLE malformed.sensor(bay_id INT,`st_marker_id` STRING,`lat` FLOAT,`lon` FLOAT,`location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>,`status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\r\n",
							"spark.sql(f\"REFRESH TABLE malformed.sensor\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_explore')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "synspdevsin23",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"import os\r\n",
							"import datetime\r\n",
							"\r\n",
							"# For testing\r\n",
							"base_path = 'abfss://datalake@mdwdostordevi6c0n.dfs.core.windows.net/data/lnd/2021_08_04_01_28_02'\r\n",
							"parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\r\n",
							"sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\r\n",
							"\r\n",
							"\r\n",
							"parkingbay_sdf = spark.read\\\r\n",
							"  .option(\"multiLine\", True)\\\r\n",
							"  .json(parkingbay_filepath)\r\n",
							"sensordata_sdf = spark.read\\\r\n",
							"  .option(\"multiLine\", True)\\\r\n",
							"  .json(sensors_filepath)\r\n",
							"\r\n",
							"display(parkingbay_sdf)\r\n",
							"display(sensordata_sdf)\r\n",
							"display(sensordata_sdf)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_standardize')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "synspdevsin23",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get dynamic pipeline parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Get folder where the REST downloads were placed\r\n",
							"infilefolder = '2021_10_05_07_58_15/'\r\n",
							"\r\n",
							"# Get pipeline name\r\n",
							"pipelinename = 'P_Ingest_MelbParkingData'\r\n",
							"\r\n",
							"# Get pipeline run id\r\n",
							"loadid = 'df2ddb82-9004-449f-84da-ae9484b446f4\"'\r\n",
							"\r\n",
							"# Get keyvault linked service name\r\n",
							"keyvaultlsname = 'Ls_KeyVault_01'\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Prepare observability mechanisms variables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"\r\n",
							"env = mssparkutils.env\r\n",
							"pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
							"\r\n",
							"# Needed to get App Insights Key\r\n",
							"appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\r\n",
							"sc.stop\r\n",
							"\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Load file path variables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": []
						},
						"source": [
							"import os\r\n",
							"import datetime\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"\r\n",
							"# For testing\r\n",
							"#infilefolder = '2021_08_17_09_23_52/'\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = token_library.getSecretWithLS( keyvaultlsname, \"datalakeaccountname\")\r\n",
							"container_name = 'datalake' # fill in your container name \r\n",
							"relative_path = 'data/lnd/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"load_id = loadid\r\n",
							"loaded_on = datetime.datetime.now()\r\n",
							"base_path = os.path.join(adls_path, infilefolder)\r\n",
							"\r\n",
							"parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\r\n",
							"print(parkingbay_filepath)\r\n",
							"sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\r\n",
							"print(sensors_filepath)\r\n",
							"#sc.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Transform: Standardize"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import ddo_transform.standardize as s\r\n",
							"\r\n",
							"# Retrieve schema\r\n",
							"parkingbay_schema = s.get_schema(\"in_parkingbay_schema\")\r\n",
							"sensordata_schema = s.get_schema(\"in_sensordata_schema\")\r\n",
							"\r\n",
							"# Read data\r\n",
							"parkingbay_sdf = spark.read\\\r\n",
							"  .schema(parkingbay_schema)\\\r\n",
							"  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingBayData\"))\\\r\n",
							"  .option(\"multiLine\", True)\\\r\n",
							"  .json(parkingbay_filepath)\r\n",
							"sensordata_sdf = spark.read\\\r\n",
							"  .schema(sensordata_schema)\\\r\n",
							"  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingSensorData\"))\\\r\n",
							"  .option(\"multiLine\", True)\\\r\n",
							"  .json(sensors_filepath)\r\n",
							"\r\n",
							"\r\n",
							"# Standardize\r\n",
							"t_parkingbay_sdf, t_parkingbay_malformed_sdf = s.standardize_parking_bay(parkingbay_sdf, load_id, loaded_on)\r\n",
							"t_sensordata_sdf, t_sensordata_malformed_sdf = s.standardize_sensordata(sensordata_sdf, load_id, loaded_on)\r\n",
							"\r\n",
							"# Insert new rows\r\n",
							"t_parkingbay_sdf.write.mode(\"append\").insertInto(\"interim.parking_bay\")\r\n",
							"t_sensordata_sdf.write.mode(\"append\").insertInto(\"interim.sensor\")\r\n",
							"\r\n",
							"# Insert bad rows\r\n",
							"t_parkingbay_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.parking_bay\")\r\n",
							"t_sensordata_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.sensor\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5. Observability: create log messages"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"parkingbay_count = t_parkingbay_sdf.count()\r\n",
							"sensordata_count = t_sensordata_sdf.count()\r\n",
							"parkingbay_malformed_count = t_parkingbay_malformed_sdf.count()\r\n",
							"sensordata_malformed_count = t_sensordata_malformed_sdf.count()\r\n",
							"\r\n",
							"\r\n",
							"final_message = f'Standardize : Completed load {pipelineruninfo}::[parkingbay_filepath::{parkingbay_filepath}]::[sensors_filepath:{sensors_filepath}]::[parkingbay_count:{parkingbay_count}]::[sensordata_count:{sensordata_count}]::[parkingbay_malformed_count:{parkingbay_malformed_count}]::[sensordata_malformed_count:{sensordata_malformed_count}]'\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 6. Observability: logging on App Insigths using OpenCensus Library"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\r\n",
							"import os\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
							"from opencensus.ext.azure.log_exporter import AzureEventHandler\r\n",
							"#from pyspark.sql.session import SparkSession\r\n",
							"from datetime import datetime\r\n",
							"\r\n",
							"# Enable App Insights\r\n",
							"aiLogger = logging.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
							"aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
							"#logger.addHandler(AzureLogHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
							"\r\n",
							"\r\n",
							"aiLogger.warning(\"Starting at: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
							"properties = {'custom_dimensions': {'pipeline': pipelinename, 'run_id': loadid, 'parking count': parkingbay_count, 'sensor count': sensordata_count}}\r\n",
							"aiLogger.warning(final_message, extra=properties)\r\n",
							"# To query this log go to the Azure Monitor and run the following kusto query (if you are using the EventHandler)\r\n",
							"#customEvents\r\n",
							"#|order by timestamp desc\r\n",
							"# To query this log go to the Azure Monitor and run the following kusto query (if you are using the LogHandler)\r\n",
							"# traces\r\n",
							"#|order by timestamp desc\r\n",
							"\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 7. Observability logging on Log Analytics workspace using Log4J"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\r\n",
							"import sys\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"env = mssparkutils.env\r\n",
							"pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
							"final_message = f'Standardize:Completed load::[parkingbay_filepath::{parkingbay_filepath}]::[sensors_filepath:{sensors_filepath}]::[parkingbay_count:{parkingbay_count}]::[sensor_count:{sensordata_count}]::[parkingbay_malformed_count:{parkingbay_malformed_count}]::[sensor_malformed_count:{sensordata_malformed_count}]'\r\n",
							"\r\n",
							"# Enable Log Analytics using Log4J\r\n",
							"log4jLogger = sc._jvm.org.apache.log4j\r\n",
							"logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
							"logger.info(pipelineruninfo)\r\n",
							"logger.info(final_message)\r\n",
							"\r\n",
							"# To query this log go to the log analytics workspace and run the following kusto query:\r\n",
							"# SparkLoggingEvent_CL\r\n",
							"# | where logger_name_s == \"ParkingSensorLogs-Standardize\"\r\n",
							"\r\n",
							"\r\n",
							"sc.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [],
						"attachments": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "synspdevsin23",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get dynamic pipeline parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Get pipeline run id\r\n",
							"loadid = 0\r\n",
							"\r\n",
							"# Get pipeline name\r\n",
							"pipelinename = 'pipeline_name'\r\n",
							"\r\n",
							"# Get keyvault linked service name\r\n",
							"keyvaultlsname = 'Ls_KeyVault_01'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Prepare observability mechanisms variables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"\r\n",
							"env = mssparkutils.env\r\n",
							"pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
							"\r\n",
							"# Needed to get App Insights Key\r\n",
							"appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Transform and load Dimension tables\r\n"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import datetime\r\n",
							"import os\r\n",
							"from pyspark.sql.functions import col, lit\r\n",
							"import ddo_transform.transform as t\r\n",
							"import ddo_transform.util as util\r\n",
							"\r\n",
							"load_id = loadid\r\n",
							"loaded_on = datetime.datetime.now()\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = token_library.getSecretWithLS(keyvaultlsname,\"datalakeaccountname\")\r\n",
							"container_name = 'datalake' # fill in your container name \r\n",
							"relative_path = 'data/dw/' # fill in your relative folder path \r\n",
							"\r\n",
							"base_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"\r\n",
							"# Read interim cleansed data\r\n",
							"parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\r\n",
							"sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\r\n",
							"\r\n",
							"# Read existing Dimensions\r\n",
							"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\r\n",
							"dim_location_sdf = spark.read.table(\"dw.dim_location\")\r\n",
							"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\r\n",
							"\r\n",
							"# Transform\r\n",
							"new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\r\n",
							"new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\r\n",
							"new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\r\n",
							"\r\n",
							"# Load\r\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_parkingbay_sdf, table_name=\"dw.dim_parking_bay\", path=os.path.join(base_path, \"dim_parking_bay\"))\r\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_location_sdf, table_name=\"dw.dim_location\", path=os.path.join(base_path, \"dim_location\"))\r\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_st_marker_sdf, table_name=\"dw.dim_st_marker\", path=os.path.join(base_path, \"dim_st_marker\"))\r\n",
							"\r\n",
							"\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Transform and load Fact tables"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Read existing Dimensions\r\n",
							"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\r\n",
							"dim_location_sdf = spark.read.table(\"dw.dim_location\")\r\n",
							"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\r\n",
							"\r\n",
							"# Process\r\n",
							"nr_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\r\n",
							"\r\n",
							"# Insert new rows\r\n",
							"nr_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5.  Observability: create log messages"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"new_dim_parkingbay_count = spark.read.table(\"dw.dim_parking_bay\").count()\r\n",
							"new_dim_location_count = spark.read.table(\"dw.dim_location\").count()\r\n",
							"new_dim_st_marker_count = spark.read.table(\"dw.dim_st_marker\").count()\r\n",
							"nr_fact_parking_count = nr_fact_parking.count()\r\n",
							"\r\n",
							"\r\n",
							"final_message = f'Transform : Completed load::[new_dim_parkingbay_count::{new_dim_parkingbay_count}]::[new_dim_location_count:{new_dim_location_count}]::[new_dim_st_marker_count:{new_dim_st_marker_count}]::[nr_fact_parking_count:{nr_fact_parking_count}]'\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 7. Observability: logging on App Insigths using OpenCensus Library"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\r\n",
							"import os\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
							"from opencensus.ext.azure.log_exporter import AzureEventHandler\r\n",
							"from pyspark.sql.session import SparkSession\r\n",
							"from datetime import datetime\r\n",
							"\r\n",
							"# Enable App Insights\r\n",
							"aiLogger = logging.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
							"aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
							"#logger.addHandler(AzureLogHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
							"\r\n",
							"\r\n",
							"aiLogger.warning(\"Starting at: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
							"properties = {'custom_dimensions': {'pipeline': pipelinename, 'run_id': loadid, 'new parking count': new_dim_parkingbay_count}}\r\n",
							"aiLogger.warning(final_message, extra=properties)\r\n",
							"# To query this log go to the Azure Monitor and run the following kusto query (if you are using the EventHandler)\r\n",
							"#customEvents\r\n",
							"#|order by timestamp desc\r\n",
							"# To query this log go to the Azure Monitor and run the following kusto query (if you are using the LogHandler)\r\n",
							"# traces\r\n",
							"#|order by timestamp desc"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 6. Observability: logging on Log Analytics workspace using Log4J"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\r\n",
							"import sys\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"env = mssparkutils.env\r\n",
							"pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
							"final_message = f'Standardize : Completed load::[new_dim_parkingbay_count:{new_dim_parkingbay_count}]::[new_dim_location_count:{new_dim_location_count}]::[new_dim_st_marker_count :{new_dim_st_marker_count }]::[nr_fact_parking_count:{nr_fact_parking_count}]'\r\n",
							"\r\n",
							"# Enable Log Analytics using Log4J\r\n",
							"log4jLogger = sc._jvm.org.apache.log4j\r\n",
							"logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
							"logger.info(final_message)\r\n",
							"# To query this log go to the log analytics workspace and run the following kusto query:\r\n",
							"# SparkLoggingEvent_CL\r\n",
							"# | where logger_name_s == \"ParkingSensorLogs-Standardize\"\r\n",
							"sc.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syndpdevsin23')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "southeastasia"
		}
	]
}